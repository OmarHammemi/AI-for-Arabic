{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <p style=\"text-align:center;\"> AI for Arabic </p>\n![](https://www.ia-challenge.tn/ChallengeAssets/images/heroImageAINC.svg)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-15T00:14:19.409163Z","iopub.execute_input":"2022-03-15T00:14:19.409469Z","iopub.status.idle":"2022-03-15T00:14:19.418223Z","shell.execute_reply.started":"2022-03-15T00:14:19.409411Z","shell.execute_reply":"2022-03-15T00:14:19.416853Z"}}},{"cell_type":"markdown","source":"# Workflow of Machine Learning project on Android\n\nSolving Machine learning Problems in a local system is only not the case but making it able to community to use is important otherwise the model is up to you only. When it is able to serve then you came to know the feedback and improvements needed to improve it. Implementing a Machine learning model in a jupyter Notebook is a very easy task. And 90 percent of times when any data science practitioner deploys his problem statement it is in the form of a website. So How can we apply it  into an Android app!.\n\n# Machine Learning(ML) Model on Android cover\n\nWorkflow of Machine Learning project on Android\nWhen we deploy machine learning on a website, the basic workflow is implementing the model in any Python IDE, extracting it using a pickle module, and with help of any web framework flask or streamlit to deploy in form of the web app. here the complete implementation from frontend to the backend is in Python.","metadata":{}},{"cell_type":"markdown","source":"**Now when deploying Machine learning in android there is a little bit of modification in the above workflow. First, we have a model, we pickle it. For implementing Android apps java is popular and working with android studio java is mostly preferred so here our frontend will depend on java and in middle we have to implement a Flask API which is our machine learning model whose output will be in JSON format(JSON is a universal format which any programming language can understand)  and through java android app we will hit at Flask API whose response is in JSON and we will parse this JSON and print it in android frontend.**\n![](https://editor.analyticsvidhya.com/uploads/46212ML%20on%20Android%20Workflow%20diagram.png)","metadata":{}},{"cell_type":"markdown","source":"# Our workflow Is :\n* Building A model\n* Building A Flask Api\n* Test Application using Postman\n* Create Android App\n* Connectivity of API to Android APP\n* Write backend logic in java","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"text-align:center;\"> Building a model </p>","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport lightgbm as lgb\nimport sklearn \nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nimport random\nimport os\nprint('\\n'.join(os.listdir(\"../input\")))\n\n# Any results you write to the current directory are saved as output.","metadata":{"execution":{"iopub.status.busy":"2022-03-15T06:20:02.949182Z","iopub.execute_input":"2022-03-15T06:20:02.949475Z","iopub.status.idle":"2022-03-15T06:20:04.079785Z","shell.execute_reply.started":"2022-03-15T06:20:02.949421Z","shell.execute_reply":"2022-03-15T06:20:04.078951Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def read_tsv(data_file):\n    text_data = list()\n    labels = list()\n    infile = open(data_file, encoding='utf-8')\n    for line in infile:\n        if not line.strip():\n            continue\n        label, text = line.split('\\t')\n        text_data.append(text)\n        labels.append(label)\n    return text_data, labels","metadata":{"execution":{"iopub.status.busy":"2022-03-15T06:20:05.733537Z","iopub.execute_input":"2022-03-15T06:20:05.733816Z","iopub.status.idle":"2022-03-15T06:20:05.738945Z","shell.execute_reply.started":"2022-03-15T06:20:05.733765Z","shell.execute_reply":"2022-03-15T06:20:05.738224Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_pos=read_tsv('../input/train_Arabic_tweets_positive_20190413.tsv')\ntrain_neg=read_tsv('../input/train_Arabic_tweets_negative_20190413.tsv')\ntest_pos=read_tsv('../input/train_Arabic_tweets_positive_20190413.tsv')\ntest_neg=read_tsv('../input/test_Arabic_tweets_negative_20190413.tsv')\n","metadata":{"execution":{"iopub.status.busy":"2022-03-15T06:20:07.150829Z","iopub.execute_input":"2022-03-15T06:20:07.151436Z","iopub.status.idle":"2022-03-15T06:20:07.432911Z","shell.execute_reply.started":"2022-03-15T06:20:07.151357Z","shell.execute_reply":"2022-03-15T06:20:07.432077Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_d=pd.DataFrame({'positive':train_pos[0],'value':'1'})\ntrain_d","metadata":{"execution":{"iopub.status.busy":"2022-03-15T06:20:08.213505Z","iopub.execute_input":"2022-03-15T06:20:08.213787Z","iopub.status.idle":"2022-03-15T06:20:08.257567Z","shell.execute_reply.started":"2022-03-15T06:20:08.213738Z","shell.execute_reply":"2022-03-15T06:20:08.256975Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"len(train_pos[0]),len(train_neg[0]),len(test_pos[0]),len(test_neg[0])","metadata":{"execution":{"iopub.status.busy":"2022-03-15T06:20:09.710133Z","iopub.execute_input":"2022-03-15T06:20:09.710409Z","iopub.status.idle":"2022-03-15T06:20:09.718869Z","shell.execute_reply.started":"2022-03-15T06:20:09.710351Z","shell.execute_reply":"2022-03-15T06:20:09.717685Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# define functions","metadata":{}},{"cell_type":"code","source":"def load(pos_train_file, neg_train_file, pos_test_file, neg_test_file):\n    pos_train_data, pos_train_labels = read_tsv(pos_train_file)\n    neg_train_data, neg_train_labels = read_tsv(neg_train_file)\n\n    pos_test_data, pos_test_labels = read_tsv(pos_test_file)\n    neg_test_data, neg_test_labels = read_tsv(neg_test_file)\n    print('------------------------------------')\n\n    sample_size = 5\n    print('{} random train tweets (positive) .... '.format(sample_size))\n    print(np.array(random.sample(pos_train_data, sample_size)))\n    print('------------------------------------')\n    print('{} random train tweets (negative) .... '.format(sample_size))\n    print(np.array(random.sample(neg_train_data, sample_size)))\n    print('------------------------------------')\n\n    x_train = pos_train_data + neg_train_data\n    y_train = pos_train_labels + neg_train_labels\n\n    x_test = pos_test_data + neg_test_data\n    y_test = pos_test_labels + neg_test_labels\n\n    print('train data size:{}\\ttest data size:{}'.format(len(y_train), len(y_test)))\n    print('train data: # of pos:{}\\t# of neg:{}\\t'.format(y_train.count('pos'), y_train.count('neg')))\n    print('test data: # of pos:{}\\t# of neg:{}\\t'.format(y_test.count('pos'), y_test.count('neg')))\n    print('------------------------------------')\n    return x_train, y_train, x_test, y_test\n","metadata":{"execution":{"iopub.status.busy":"2022-03-15T06:20:12.105240Z","iopub.execute_input":"2022-03-15T06:20:12.105610Z","iopub.status.idle":"2022-03-15T06:20:12.113448Z","shell.execute_reply.started":"2022-03-15T06:20:12.105560Z","shell.execute_reply":"2022-03-15T06:20:12.112348Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"###############################################################\n\ndef do_sa(n, my_classifier, name, my_data):\n    x_train, y_train, x_test, y_test = my_data\n    print('parameters')\n    print('n grams:', n)\n    print('classifier:', my_classifier.__class__.__name__)\n    print('------------------------------------')\n\n    pipeline = Pipeline([\n        ('vect', TfidfVectorizer(min_df=0.001, max_df=0.95,\n                                 #min_df=0.0001, max_df=0.95,\n                                 analyzer='word', lowercase=False,\n                                 ngram_range=(1, n))),\n        ('clf', my_classifier),\n    ])\n\n    pipeline.fit(x_train, y_train)\n    feature_names = pipeline.named_steps['vect'].get_feature_names()\n\n    y_predicted = pipeline.predict(x_test)\n\n    # Print the classification report\n    print(metrics.classification_report(y_test, y_predicted,\n                                        target_names=['pos', 'neg']))\n\n    # Print the confusion matrix\n    cm = metrics.confusion_matrix(y_test, y_predicted)\n    print(cm)\n    print('# of features:', len(feature_names))\n    print('sample of features:', random.sample(feature_names, 40))\n    accuracy = accuracy_score(y_test, y_predicted)\n    precision = precision_score(y_test, y_predicted, average='weighted')\n    recall =  recall_score(y_test, y_predicted, average='weighted')\n    return name, n, accuracy, precision, recall\n","metadata":{"execution":{"iopub.status.busy":"2022-03-14T21:51:57.550842Z","iopub.execute_input":"2022-03-14T21:51:57.551424Z","iopub.status.idle":"2022-03-14T21:51:57.573982Z","shell.execute_reply.started":"2022-03-14T21:51:57.551375Z","shell.execute_reply":"2022-03-14T21:51:57.573169Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Setup experiments ","metadata":{}},{"cell_type":"code","source":"ngrams = (1, 2, 3)\nresults = []\npos_training = '../input/train_Arabic_tweets_positive_20190413.tsv'\nneg_training = '../input/train_Arabic_tweets_negative_20190413.tsv'\n\npos_testing = '../input/test_Arabic_tweets_positive_20190413.tsv'\nneg_testing = '../input/test_Arabic_tweets_negative_20190413.tsv'\n\nclassifiers = [LinearSVC(), SVC(), MultinomialNB(),XGBClassifier(),lgb.LGBMClassifier(),\n               BernoulliNB(), SGDClassifier(), DecisionTreeClassifier(max_depth=5),\n               RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n               KNeighborsClassifier(3)\n               ]\nfor g in ngrams:\n    dataset = load(pos_training, neg_training, pos_testing, neg_testing)\n    for alg in classifiers:\n        alg_name = alg.__class__.__name__\n        r = do_sa(g, alg, alg_name, dataset)\n        results.append(r)\n        ","metadata":{"execution":{"iopub.status.busy":"2022-03-14T21:51:57.575366Z","iopub.execute_input":"2022-03-14T21:51:57.575846Z","iopub.status.idle":"2022-03-14T22:05:28.301208Z","shell.execute_reply.started":"2022-03-14T21:51:57.575779Z","shell.execute_reply":"2022-03-14T22:05:28.300042Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":" #  Results Summary","metadata":{}},{"cell_type":"code","source":"print('{0:25}{1:10}{2:10}{3:10}{4:10}'.format('algorithm', 'ngram', 'accuracy', 'precision', 'recall'))\nprint('---------------------------------------------------------------------')\nfor r in results:\n    print('{0:25}{1:10}{2:10.3f}{3:10.3f}{4:10.3f}'.format(r[0], r[1], r[2], r[3], r[4]))","metadata":{"execution":{"iopub.status.busy":"2022-03-14T22:05:28.302891Z","iopub.execute_input":"2022-03-14T22:05:28.303500Z","iopub.status.idle":"2022-03-14T22:05:28.315850Z","shell.execute_reply.started":"2022-03-14T22:05:28.303367Z","shell.execute_reply":"2022-03-14T22:05:28.314407Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Deep Learning Approach\nGiven that the Random Forest and LinearSVC Classifier models wasn't generalizing well for other datasets (possibly overfitting), I decided to try a DL approach using a pretrained model (i.e: increasing the dataset as a way of overcoming overfitting). For that I chose to use the Arabic-BERT model By Ali Safaya.\nThe models were pretrained on ~8.2 Billion words:\n\nArabic version of OSCAR (unshuffled version of the corpus) - filtered from Common Crawl\nRecent dump of Arabic Wikipedia","metadata":{}},{"cell_type":"code","source":"#import torch\n#\n#if torch.cuda.is_available():       \n #   device = torch.device(\"cuda\")\n #   print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n#    print('Device name:', torch.cuda.get_device_name(0))\n#\n#else:\n #   print('No GPU available, using the CPU instead.')\n #   device = torch.device(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-03-14T22:35:56.464630Z","iopub.execute_input":"2022-03-14T22:35:56.464959Z","iopub.status.idle":"2022-03-14T22:35:56.469576Z","shell.execute_reply.started":"2022-03-14T22:35:56.464904Z","shell.execute_reply":"2022-03-14T22:35:56.468553Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"#from sklearn.feature_extraction.text import CountVectorizer\n#from sklearn.linear_model import LogisticRegression\n#from sklearn.pipeline import #","metadata":{"execution":{"iopub.status.busy":"2022-03-14T22:36:14.471056Z","iopub.execute_input":"2022-03-14T22:36:14.471330Z","iopub.status.idle":"2022-03-14T22:36:14.475312Z","shell.execute_reply.started":"2022-03-14T22:36:14.471282Z","shell.execute_reply":"2022-03-14T22:36:14.474428Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"text-align:center;\"> Building a Flask API </p>\n![](https://miro.medium.com/max/1103/1*pu5-oy7xcIJafXim7RR_9w.png)","metadata":{}},{"cell_type":"markdown","source":"**A user will enter information in the form and while submitting the form it will receive the POST request. And on making a post request Flask API will accept the data entered by a user and pass it to the machine learning model which will predict the output class. The predicted class we will pass to the android app in form of JSON.**\n\n","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"text-align:center;\"> Test Application using Postman</p>\n![](https://editor.analyticsvidhya.com/uploads/11389POSTman_results.png)\nPostman is an automatic and interactive tool used to verify APIs of your project. It is Google chrome App that connects with HTTP API. It works at the backend and allows you to check that your API is working fine as per our requirements. By providing the URL of your running flask API and inserting data in the key and value section you can hit your API and get the desired response.","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"text-align:center;\">Create Android App</p>\n![](https://editor.analyticsvidhya.com/uploads/62228Android_new_project.png)","metadata":{}},{"cell_type":"markdown","source":"# Create Android UI\n**We know that UI is always created in an XML file. Open the XML file named activity main and here we will build a complete frontend UI. You can use the below code snippet. First, we have given the title of the project, three input fields for respective columns, and one button to submit and get results.**","metadata":{}},{"cell_type":"markdown","source":"# Connectivity of API to Android APP","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:51:13.689774Z","iopub.execute_input":"2022-03-15T05:51:13.690091Z","iopub.status.idle":"2022-03-15T05:51:13.696147Z","shell.execute_reply.started":"2022-03-15T05:51:13.690026Z","shell.execute_reply":"2022-03-15T05:51:13.695151Z"}}},{"cell_type":"markdown","source":"**Now you have to write its backend working in java. The logic we have to implement is you will take the inputs from the android app, hit the API, and the response from the API display back in the Android app. So, there is one problem that the API we have implemented is running locally on your system which the Android app cannot detect. so we need to deploy our API online and we will use Heroku for this task.**","metadata":{}},{"cell_type":"markdown","source":"# Deploy API to Heroku\n\n**Login to Heroku and create New app by giving it a unique name. deploy your GUI using Heroku CLI or GitHub.**","metadata":{}},{"cell_type":"markdown","source":"# Connect API to Internet","metadata":{}},{"cell_type":"markdown","source":"**To hit API we need one library named Volley. So to install Volley visit In Gradle scripts in your project directory, and open the build Gradle file and write below one line of code that will install the required library. As you click on sync now on the top right it will start installing required libraries in the project directory.**\n![](https://editor.analyticsvidhya.com/uploads/96786android_install_library.png)","metadata":{}},{"cell_type":"markdown","source":"# Write backend logic in java\n**We have designed our UI, Now we need to write backend logic to accept data from the frontend in a java file. The flow is where we have to accept all the three values from the android app and when it click predict button we have to hit API and ret JSON response. keeping all the imports as it is, you can follow the below code from class in the Main Activity java file.**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}